{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b50d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Functions for snow tutorial notebooks\n",
    "#\n",
    "# In Python a module is just a collection of functions in a file with\n",
    "# a .py extension.\n",
    "#\n",
    "# Functions are defined using:\n",
    "#\n",
    "# def function_name(argument1, arguments2,... keyword_arg1=some_variable) \n",
    "#     '''A docstring explaining what the function does and what\n",
    "#        arguments it expectes.\n",
    "#     '''\n",
    "#     <commands>\n",
    "#     return some_value  # Not required unless you need to return a value\n",
    "#\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta\n",
    "import pyproj\n",
    "import requests\n",
    "import json\n",
    "from statistics import mean\n",
    "from xml.etree import ElementTree as ET\n",
    "import os\n",
    "import pprint\n",
    "import shutil\n",
    "import zipfile\n",
    "import io\n",
    "import time\n",
    "import itertools\n",
    "from urllib.parse import urlparse\n",
    "import netrc\n",
    "import base64\n",
    "from urllib.error import HTTPError, URLError\n",
    "from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "def granule_info(data_dict):\n",
    "    '''\n",
    "    Prints number of granules based on inputted data set short name, version, bounding box, and temporal range. Queries the CMR and pages over results.\n",
    "    \n",
    "    data_dict - a dictionary with the following CMR keywords:\n",
    "    'short_name',\n",
    "    'version',\n",
    "    'bounding_box',\n",
    "    'temporal'\n",
    "    '''\n",
    "    # set CMR API endpoint for granule search\n",
    "    granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "    \n",
    "    # add page size and page num to dictionary\n",
    "    data_dict['page_size'] = 2000\n",
    "    data_dict['page_num'] = 1\n",
    "    \n",
    "    granules = []\n",
    "    headers={'Accept': 'application/json'}\n",
    "    while True:\n",
    "        response = requests.get(granule_search_url, params=data_dict, headers=headers)\n",
    "        results = json.loads(response.content)\n",
    "\n",
    "        if len(results['feed']['entry']) == 0:\n",
    "            # Out of results, so break out of loop\n",
    "            data_dict['page_num'] -= 1\n",
    "            break\n",
    "\n",
    "        # Collect results and increment page_num\n",
    "        granules.extend(results['feed']['entry'])\n",
    "        data_dict['page_num'] += 1\n",
    "    \n",
    "    # calculate granule size \n",
    "    granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "    print('There are', len(granules), 'files of', data_dict['short_name'], 'version', data_dict['version'], 'over my area and time of interest.')\n",
    "    print(f'The average size of each file is {mean(granule_sizes):.2f} MB and the total size of all {len(granules)} granules is {sum(granule_sizes):.2f} MB')\n",
    "    return len(granules)\n",
    "\n",
    "def merge_intervals(intervals):\n",
    "    sorted_by_lower_bound = sorted(intervals, key=lambda tup: tup[0])\n",
    "    merged = []\n",
    "    for higher in sorted_by_lower_bound:\n",
    "        if not merged:\n",
    "            merged.append(higher)\n",
    "        else:\n",
    "            lower = merged[-1]\n",
    "            # test for intersection between lower and higher:\n",
    "            # we know via sorting that lower[0] <= higher[0]\n",
    "            if higher[0] <= lower[1]:\n",
    "                upper_bound = max(lower[1], higher[1])\n",
    "                merged[-1] = (lower[0], upper_bound)  # replace by merged interval\n",
    "            else:\n",
    "                merged.append(higher)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def time_overlap(data_dict):\n",
    "    '''\n",
    "    Prints dataframe with file names, dataset_id, start date, and end date for all files that overlap in temporal range across all data sets in dictionary\n",
    "    \n",
    "    data_dict - a dictionary with the following CMR keywords:\n",
    "    'short_name',\n",
    "    'version',\n",
    "    'bounding_box',\n",
    "    'temporal'\n",
    "    '''\n",
    "    # set CMR API endpoint for granule search\n",
    "    granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "    headers= {'Accept': 'application/json'}\n",
    "    \n",
    "    # Create dataframe with identifiers and temporal ranges\n",
    "    granules = []\n",
    "    column_names = ['dataset_id', 'short_name','version', 'producer_granule_id', 'start_date', 'end_date']\n",
    "    df = pd.DataFrame(columns = column_names)\n",
    "    for k, v in data_dict.items(): \n",
    "        # add page size and page num to dictionary\n",
    "        data_dict[k]['page_size'] = 2000\n",
    "        data_dict[k]['page_num'] = 1\n",
    "\n",
    "        while True:\n",
    "            response = requests.get(granule_search_url, params=data_dict[k], headers=headers)\n",
    "            results = json.loads(response.content)\n",
    "            if len(results['feed']['entry']) == 0:\n",
    "                # Out of results, so break out of loop\n",
    "                data_dict[k]['page_num'] -= 1\n",
    "                break\n",
    "            # Collect results and increment page_num\n",
    "            granules.extend(results['feed']['entry'])\n",
    "            data_dict[k]['page_num'] += 1\n",
    "        # compile lists from granule metadata\n",
    "        dataset_id = [granule['dataset_id'] for granule in granules]\n",
    "        title = [granule['title'] for granule in granules]\n",
    "        producer_granule_id = [granule['producer_granule_id'] for granule in granules]\n",
    "        start_date = [granule['time_start'] for granule in granules]\n",
    "        end_date = [granule['time_end'] for granule in granules]\n",
    "\n",
    "    # split title to feed short_name and version lists \n",
    "    title_split = [i.split(':') for i in title]\n",
    "    name = [i[1] for i in title_split]\n",
    "    name_split = [i.split('.') for i in name]\n",
    "\n",
    "    df['dataset_id'] = dataset_id\n",
    "    df['short_name'] = [i[0] for i in name_split]\n",
    "    df['version'] = [i[1] for i in name_split]\n",
    "    df['producer_granule_id'] = producer_granule_id\n",
    "    df['start_date'] = start_date\n",
    "    df['end_date'] = end_date\n",
    "    \n",
    "    # Convert state time to integers \n",
    "    df['start_int'] = pd.DatetimeIndex(df['start_date']).astype(np.int64)\n",
    "    df['end_int'] = pd.DatetimeIndex(df['end_date']).astype(np.int64)\n",
    "    \n",
    "    merged = merge_intervals(zip(df['start_int'], df['end_int']))\n",
    "    df['overlap_group'] = df['start_int'].apply(lambda x: next(i for i, m in enumerate(merged) if m[0] <= x <= m[1]))\n",
    "    \n",
    "    # Find each unique value in overlap_group\n",
    "    len_datasets = len(df.dataset_id.unique())\n",
    "    len_groups = len(df.overlap_group.unique())\n",
    "    unique_group = list(df.overlap_group.unique())\n",
    "\n",
    "    # Loop over each overlap group\n",
    "    tempdf = df.copy()\n",
    "    \n",
    "    for i in range(len_groups):\n",
    "        tempdf = df.copy()\n",
    "        # Filter rows corresponding to unique_group value\n",
    "        filter_df = tempdf.loc[tempdf['overlap_group'] == unique_group[i]]  \n",
    "        # If not all datasets exist, remove this group from our main tempdf\n",
    "        filter_len_datasets = len(filter_df.dataset_id.unique())\n",
    "        if filter_len_datasets < len_datasets: df = df.loc[df.overlap_group != unique_group[i]]\n",
    "    \n",
    "    df = df.drop(columns=['start_int', 'end_int', 'overlap_group'])\n",
    "    return df\n",
    "\n",
    "def get_username():\n",
    "    username = ''\n",
    "\n",
    "    # For Python 2/3 compatibility:\n",
    "    try:\n",
    "        do_input = raw_input  # noqa\n",
    "    except NameError:\n",
    "        do_input = input\n",
    "\n",
    "    while not username:\n",
    "        try:\n",
    "            username = do_input('Earthdata username: ')\n",
    "        except KeyboardInterrupt:\n",
    "            quit()\n",
    "    return username\n",
    "\n",
    "\n",
    "def get_password():\n",
    "    password = ''\n",
    "    while not password:\n",
    "        try:\n",
    "            password = getpass('password: ')\n",
    "        except KeyboardInterrupt:\n",
    "            quit()\n",
    "    return password\n",
    "\n",
    "def get_credentials(url):\n",
    "    URS_URL = 'https://urs.earthdata.nasa.gov'\n",
    "    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n",
    "    credentials = None\n",
    "    errprefix = ''\n",
    "    try:\n",
    "        info = netrc.netrc()\n",
    "        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n",
    "        errprefix = 'netrc error: '\n",
    "    except Exception as e:\n",
    "        if (not ('No such file' in str(e))):\n",
    "            print('netrc error: {0}'.format(str(e)))\n",
    "        username = None\n",
    "        password = None\n",
    "\n",
    "    while not credentials:\n",
    "        if not username:\n",
    "            username = get_username()\n",
    "            password = get_password()\n",
    "        credentials = '{0}:{1}'.format(username, password)\n",
    "        credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n",
    "\n",
    "        if url:\n",
    "            try:\n",
    "                req = Request(url)\n",
    "                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n",
    "                opener = build_opener(HTTPCookieProcessor())\n",
    "                opener.open(req)\n",
    "            except HTTPError:\n",
    "                print(errprefix + 'Incorrect username or password')\n",
    "                errprefix = ''\n",
    "                credentials = None\n",
    "                username = None\n",
    "                password = None\n",
    "\n",
    "    return credentials\n",
    "\n",
    "def cmr_filter_urls(search_results):\n",
    "    \"\"\"Select only the desired data files from CMR response.\"\"\"\n",
    "    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n",
    "        return []\n",
    "\n",
    "    entries = [e['links']\n",
    "               for e in search_results['feed']['entry']\n",
    "               if 'links' in e]\n",
    "    # Flatten \"entries\" to a simple list of links\n",
    "    links = list(itertools.chain(*entries))\n",
    "\n",
    "    urls = []\n",
    "    unique_filenames = set()\n",
    "    for link in links:\n",
    "        if 'href' not in link:\n",
    "            # Exclude links with nothing to download\n",
    "            continue\n",
    "        if 'inherited' in link and link['inherited'] is True:\n",
    "            # Why are we excluding these links?\n",
    "            continue\n",
    "        if 'rel' in link and 'data#' not in link['rel']:\n",
    "            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n",
    "            continue\n",
    "        if 'title' in link and 'opendap' in link['title'].lower():\n",
    "            # Exclude OPeNDAP links--they are responsible for many duplicates\n",
    "            # This is a hack; when the metadata is updated to properly identify\n",
    "            # non-datapool links, we should be able to do this in a non-hack way\n",
    "            continue\n",
    "\n",
    "        filename = link['href'].split('/')[-1]\n",
    "        if filename in unique_filenames:\n",
    "            # Exclude links with duplicate filenames (they would overwrite)\n",
    "            continue\n",
    "        unique_filenames.add(filename)\n",
    "\n",
    "        urls.append(link['href'])\n",
    "\n",
    "    return urls\n",
    "\n",
    "def build_cmr_query_url(short_name, version, time_start, time_end,\n",
    "                        bounding_box=None, polygon=None,\n",
    "                        filename_filter=None):\n",
    "    params = '&short_name={0}'.format(short_name)\n",
    "    params += version\n",
    "    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n",
    "    if polygon:\n",
    "        params += '&polygon={0}'.format(polygon)\n",
    "    elif bounding_box:\n",
    "        params += '&bounding_box={0}'.format(bounding_box)\n",
    "    if filename_filter:\n",
    "        option = '&options[producer_granule_id][pattern]=true'\n",
    "        params += '&producer_granule_id[]={0}{1}'.format(filename_filter, option)\n",
    "    return CMR_FILE_URL + params\n",
    "\n",
    "def cmr_download(urls):\n",
    "    \"\"\"Download files from list of urls.\"\"\"\n",
    "    URS_URL = 'https://urs.earthdata.nasa.gov'\n",
    "    if not urls:\n",
    "        return\n",
    "\n",
    "    url_count = len(urls)\n",
    "    print('Downloading {0} files...'.format(url_count))\n",
    "    credentials = None\n",
    "\n",
    "    for index, url in enumerate(urls, start=1):\n",
    "        if not credentials and urlparse(url).scheme == 'https':\n",
    "            credentials = get_credentials(url)\n",
    "\n",
    "        filename = url.split('/')[-1]\n",
    "        filename = 'nsidc_api_output.zip' if filename.startswith('request') else filename\n",
    "        print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n",
    "                                    url_count,\n",
    "                                    filename))\n",
    "\n",
    "        try:\n",
    "            # In Python 3 we could eliminate the opener and just do 2 lines:\n",
    "            # resp = requests.get(url, auth=(username, password))\n",
    "            # open(filename, 'wb').write(resp.content)\n",
    "            req = Request(url)\n",
    "            if credentials:\n",
    "                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n",
    "            opener = build_opener(HTTPCookieProcessor())\n",
    "            data = opener.open(req).read()\n",
    "            open(filename, 'wb').write(data)\n",
    "        except HTTPError as e:\n",
    "            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n",
    "        except URLError as e:\n",
    "            print('URL error: {0}'.format(e.reason))\n",
    "        except IOError:\n",
    "            raise\n",
    "        except KeyboardInterrupt:\n",
    "            quit()\n",
    "\n",
    "\n",
    "def print_service_options(data_dict, response):\n",
    "    '''\n",
    "    Prints the available subsetting, reformatting, and reprojection services available based on inputted data set name, version, and Earthdata Login username and       password. \n",
    "    \n",
    "    data_dict - a dictionary with the following keywords:\n",
    "    'short_name',\n",
    "    'version',\n",
    "    'uid',\n",
    "    'pswd'\n",
    "    '''\n",
    "\n",
    "    root = ET.fromstring(response.content)\n",
    "\n",
    "    #collect lists with each service option\n",
    "    subagent = [subset_agent.attrib for subset_agent in root.iter('SubsetAgent')]\n",
    "\n",
    "    # variable subsetting\n",
    "    variables = [SubsetVariable.attrib for SubsetVariable in root.iter('SubsetVariable')]  \n",
    "    variables_raw = [variables[i]['value'] for i in range(len(variables))]\n",
    "    variables_join = [''.join(('/',v)) if v.startswith('/') == False else v for v in variables_raw] \n",
    "    variable_vals = [v.replace(':', '/') for v in variables_join]\n",
    "\n",
    "    # reformatting\n",
    "    formats = [Format.attrib for Format in root.iter('Format')]\n",
    "    format_vals = [formats[i]['value'] for i in range(len(formats))]\n",
    "    if format_vals : format_vals.remove('')\n",
    "\n",
    "    # reprojection options\n",
    "    projections = [Projection.attrib for Projection in root.iter('Projection')]\n",
    "    proj_vals = []\n",
    "    for i in range(len(projections)):\n",
    "        if (projections[i]['value']) != 'NO_CHANGE' :\n",
    "            proj_vals.append(projections[i]['value'])\n",
    "\n",
    "    #print service information depending on service availability and select service options\n",
    "    print('Services available for', data_dict['short_name'],':')\n",
    "    print()\n",
    "    if len(subagent) < 1 :\n",
    "            print('No customization services available.')\n",
    "    else:\n",
    "        subdict = subagent[0]\n",
    "        if subdict['spatialSubsetting'] == 'true':\n",
    "            print('Bounding box subsetting')\n",
    "        if subdict['spatialSubsettingShapefile'] == 'true':\n",
    "            print('Shapefile subsetting')\n",
    "        if subdict['temporalSubsetting'] == 'true':\n",
    "            print('Temporal subsetting')\n",
    "        if len(variable_vals) > 0:\n",
    "            print('Variable subsetting')\n",
    "        if len(format_vals) > 0 :\n",
    "            print('Reformatting to the following options:', format_vals)\n",
    "        if len(proj_vals) > 0 : \n",
    "            print('Reprojection to the following options:', proj_vals)\n",
    "\n",
    "    \n",
    "\n",
    "            \n",
    "def request_data(param_dict,session):\n",
    "    '''\n",
    "    Request data from NSIDC's API based on inputted key-value-pairs from param_dict. \n",
    "    Different request methods depending on 'async' or 'sync' options.\n",
    "    \n",
    "    In addition to param_dict, input Earthdata login `uid` and `pswd`.\n",
    "    '''\n",
    "    \n",
    "    # Create an output folder if the folder does not already exist.\n",
    "    path = str(os.getcwd() + '/Outputs')\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    # Define base URL\n",
    "    base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n",
    "    \n",
    "    # Different access methods depending on request mode:\n",
    "\n",
    "    if param_dict['request_mode'] == 'async':\n",
    "        request = session.get(base_url, params=param_dict)\n",
    "        print('Request HTTP response: ', request.status_code)\n",
    "\n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "        request.raise_for_status()\n",
    "        print()\n",
    "        print('Order request URL: ', request.url)\n",
    "        print()\n",
    "        esir_root = ET.fromstring(request.content)\n",
    "        #print('Order request response XML content: ', request.content)\n",
    "\n",
    "        #Look up order ID\n",
    "        orderlist = []   \n",
    "        for order in esir_root.findall(\"./order/\"):\n",
    "            orderlist.append(order.text)\n",
    "        orderID = orderlist[0]\n",
    "        print('order ID: ', orderID)\n",
    "\n",
    "        #Create status URL\n",
    "        statusURL = base_url + '/' + orderID\n",
    "        print('status URL: ', statusURL)\n",
    "\n",
    "        #Find order status\n",
    "        request_response = session.get(statusURL)    \n",
    "        print('HTTP response from order response URL: ', request_response.status_code)\n",
    "\n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "        request_response.raise_for_status()\n",
    "        request_root = ET.fromstring(request_response.content)\n",
    "        statuslist = []\n",
    "        for status in request_root.findall(\"./requestStatus/\"):\n",
    "            statuslist.append(status.text)\n",
    "        status = statuslist[0]\n",
    "        #print('Data request is submitting...')\n",
    "        print()\n",
    "        print('Initial request status is ', status)\n",
    "        print()\n",
    "\n",
    "        #Continue loop while request is still processing\n",
    "        loop_response = session.get(statusURL)\n",
    "        loop_root = ET.fromstring(loop_response.content)\n",
    "        while status == 'pending' or status == 'processing': \n",
    "            print('Status is not complete. Trying again.')\n",
    "            time.sleep(10)\n",
    "            loop_response = session.get(statusURL)\n",
    "\n",
    "            # Raise bad request: Loop will stop for bad response code.\n",
    "            loop_response.raise_for_status()\n",
    "            loop_root = ET.fromstring(loop_response.content)\n",
    "\n",
    "            #find status\n",
    "            statuslist = []\n",
    "            for status in loop_root.findall(\"./requestStatus/\"):\n",
    "                statuslist.append(status.text)\n",
    "            status = statuslist[0]\n",
    "            print('Retry request status is: ', status)\n",
    "            if status == 'pending' or status == 'processing':\n",
    "                continue\n",
    "\n",
    "        #Order can either complete, complete_with_errors, or fail:\n",
    "        # Provide complete_with_errors error message:\n",
    "        if status == 'failed':\n",
    "            messagelist = []\n",
    "            for message in loop_root.findall(\"./processInfo/\"):\n",
    "                messagelist.append(message.text)\n",
    "            print('error messages:')\n",
    "            pprint.pprint(messagelist)\n",
    "            print()\n",
    "\n",
    "        # Download zipped order if status is complete or complete_with_errors\n",
    "        if status == 'complete' or status == 'complete_with_errors':\n",
    "            downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/' + orderID + '.zip'\n",
    "            print('Zip download URL: ', downloadURL)\n",
    "            print('Beginning download of zipped output...')\n",
    "            zip_response = session.get(downloadURL)\n",
    "            # Raise bad request: Loop will stop for bad response code.\n",
    "            zip_response.raise_for_status()\n",
    "            with zipfile.ZipFile(io.BytesIO(zip_response.content)) as z:\n",
    "                z.extractall(path)\n",
    "            print('Data request is complete.')\n",
    "        else: print('Request failed.')\n",
    "\n",
    "    else:\n",
    "        print('Requesting...')\n",
    "        request = session.get(s.url,auth=(uid,pswd))\n",
    "        print('HTTP response from order response URL: ', request.status_code)\n",
    "        request.raise_for_status()\n",
    "        d = request.headers['content-disposition']\n",
    "        fname = re.findall('filename=(.+)', d)\n",
    "        dirname = os.path.join(path,fname[0].strip('\\\"'))\n",
    "        print('Downloading...')\n",
    "        open(dirname, 'wb').write(request.content)\n",
    "        print('Data request is complete.')\n",
    "\n",
    "        # Unzip outputs\n",
    "        for z in os.listdir(path): \n",
    "            if z.endswith('.zip'): \n",
    "                zip_name = path + \"/\" + z \n",
    "                zip_ref = zipfile.ZipFile(zip_name) \n",
    "                zip_ref.extractall(path) \n",
    "                zip_ref.close() \n",
    "                os.remove(zip_name)             \n",
    "\n",
    "                \n",
    "def clean_folder():\n",
    "    '''\n",
    "    Cleans up output folder by removing individual granule folders. \n",
    "    \n",
    "    '''\n",
    "    path = str(os.getcwd() + '/Outputs')\n",
    "    \n",
    "    for root, dirs, files in os.walk(path, topdown=False):\n",
    "        for file in files:\n",
    "            try:\n",
    "                shutil.move(os.path.join(root, file), path)\n",
    "            except OSError:\n",
    "                pass\n",
    "        for name in dirs:\n",
    "            os.rmdir(os.path.join(root, name))    \n",
    "            \n",
    "            \n",
    "def load_icesat2_as_dataframe(filepath, VARIABLES):\n",
    "    '''\n",
    "    Load points from an ICESat-2 granule 'gt<beam>' groups as DataFrame of points. Uses VARIABLES mapping\n",
    "    to select subset of '/gt<beam>/...' variables  (Assumes these variables share dimensions)\n",
    "    Arguments:\n",
    "        filepath to ATL0# granule\n",
    "    '''\n",
    "    \n",
    "    ds = h5py.File(filepath, 'r')\n",
    "\n",
    "    # Get dataproduct name\n",
    "    dataproduct = ds.attrs['identifier_product_type'].decode()\n",
    "    # Convert variable paths to 'Path' objects for easy manipulation\n",
    "    variables = [Path(v) for v in VARIABLES[dataproduct]]\n",
    "    # Get set of beams to extract individially as dataframes combining in the end\n",
    "    beams = {list(v.parents)[-2].name for v in variables}\n",
    "    \n",
    "    dfs = []\n",
    "    for beam in beams:\n",
    "        data_dict = {}\n",
    "        beam_variables = [v for v in variables if beam in str(v)]\n",
    "        for variable in beam_variables:\n",
    "            # Use variable 'name' as column name. Beam will be specified in 'beam' column\n",
    "            column = variable.name\n",
    "            variable = str(variable)\n",
    "            try:\n",
    "                values = ds[variable][:]\n",
    "                # Convert invalid data to np.nan (only for float columns)\n",
    "                if 'float' in str(values.dtype):\n",
    "                    if 'valid_min' in ds[variable].attrs:\n",
    "                        values[values < ds[variable].attrs['valid_min']] = np.nan\n",
    "                    if 'valid_max' in ds[variable].attrs:\n",
    "                        values[values > ds[variable].attrs['valid_max']] = np.nan\n",
    "                    if '_FillValue' in ds[variable].attrs:\n",
    "                        values[values == ds[variable].attrs['_FillValue']] = np.nan\n",
    "                    \n",
    "                data_dict[column] = values\n",
    "            except KeyError:\n",
    "                print(f'Variable {variable} not found in {filepath}. Likely an empty granule.')\n",
    "                raise\n",
    "                \n",
    "        df = pd.DataFrame.from_dict(data_dict)\n",
    "        df['beam'] = beam\n",
    "        dfs.append(df)\n",
    "        \n",
    "    df = pd.concat(dfs, sort=True)\n",
    "    # Add filename column for book-keeping and reset index\n",
    "    df['filename'] = Path(filepath).name\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_gdf(df):\n",
    "    '''\n",
    "    Converts a DataFrame of points with 'longitude' and 'latitude' columns to a\n",
    "    GeoDataFrame\n",
    "    '''\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gpd.points_from_xy(df.longitude, df.latitude),\n",
    "        crs={'init': 'epsg:4326'},\n",
    "    )\n",
    "\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def convert_delta_time(delta_time):\n",
    "    '''\n",
    "    Convert ICESat-2 'delta_time' parameter to UTC datetime\n",
    "    '''\n",
    "    EPOCH = datetime(2018, 1, 1, 0, 0, 0)\n",
    "    \n",
    "    utc_datetime = EPOCH + timedelta(seconds=delta_time)\n",
    "\n",
    "    return utc_datetime\n",
    "\n",
    "\n",
    "# def compute_distance(df):\n",
    "#     '''\n",
    "#     Calculates along track distance for each point within the 'gt1l', 'gt2l', and 'gt3l' beams, beginning with first beam index. \n",
    "\n",
    "#     Arguments:\n",
    "#         df: DataFrame with icesat-2 data\n",
    "\n",
    "#     Returns:\n",
    "#         add_dist added as new column to initial df\n",
    "#     '''\n",
    "\n",
    "#     beam_1 = df[df['beam'] == 'gt1l']\n",
    "#     beam_2 = df[df['beam'] == 'gt2l']\n",
    "#     beam_3 = df[df['beam'] == 'gt3l']\n",
    "\n",
    "#     add_dist = []\n",
    "#     add_dist.append(beam_1.height_segment_length_seg.values[0])\n",
    "\n",
    "#     for i in range(1, len(beam_1)): \n",
    "#         add_dist.append(add_dist[i-1] + beam_1.height_segment_length_seg.values[i])\n",
    "\n",
    "#     add_dist_se = pd.Series(add_dist)\n",
    "#     beam_1.insert(loc=0, column='add_dist', value=add_dist_se.values)\n",
    "#     beam_1\n",
    "\n",
    "#     add_dist = []\n",
    "#     add_dist.append(beam_2.height_segment_length_seg.values[0])\n",
    "\n",
    "#     for i in range(1, len(beam_2)): \n",
    "#         add_dist.append(add_dist[i-1] + beam_2.height_segment_length_seg.values[i])\n",
    "\n",
    "#     add_dist_se = pd.Series(add_dist)\n",
    "#     beam_2.insert(loc=0, column='add_dist', value=add_dist_se.values)\n",
    "#     beam_2\n",
    "\n",
    "#     add_dist = []\n",
    "#     add_dist.append(beam_3.height_segment_length_seg.values[0])\n",
    "\n",
    "#     for i in range(1, len(beam_3)): \n",
    "#         add_dist.append(add_dist[i-1] + beam_3.height_segment_length_seg.values[i])\n",
    "\n",
    "#     add_dist_se = pd.Series(add_dist)\n",
    "#     beam_3.insert(loc=0, column='add_dist', value=add_dist_se.values)\n",
    "#     beam_3\n",
    "\n",
    "#     beams = [beam_1,beam_2,beam_3]\n",
    "#     df = pd.concat(beams,ignore_index=True)\n",
    "    \n",
    "#     return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NSM_env",
   "language": "python",
   "name": "nsm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
